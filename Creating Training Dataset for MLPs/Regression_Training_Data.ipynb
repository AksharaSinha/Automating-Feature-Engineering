{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression Training Data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBM6qw4DRpY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "bed2a583-e062-4505-c085-4c037c908e12"
      },
      "source": [
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from scipy.io import arff \n",
        "import pandas as pd  \n",
        "import numpy as np  \n",
        "import matplotlib.pyplot as plt  \n",
        "import seaborn as seabornInstance \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "from sklearn import tree\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "import collections\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.io import arff \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzToZSPNhtfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(list_of_files):\n",
        "  \"\"\"creates three lists and appends all the datasets and their Y variable respectively \"\"\"\n",
        "  X_ls = []\n",
        "  Y_ls = []\n",
        "  Y_label = []\n",
        "  \n",
        "  for filename in list_of_files:\n",
        "    df = pd.read_csv(filename)\n",
        "    df = df.dropna()\n",
        "\n",
        "    # getting Y_ls\n",
        "    Y = df.iloc[:,-1]\n",
        "    Y = np.array(Y).reshape(-1, 1)\n",
        "    scaler = MinMaxScaler()\n",
        "    Y = pd.DataFrame(scaler.fit_transform(Y))\n",
        "    Y_ls.append(Y)\n",
        "    \n",
        "    # getting Y_label using k-means\n",
        "    yd = DataFrame(Y)\n",
        "    y = yd.values\n",
        "    kmeans = KMeans(n_clusters=6).fit(y)\n",
        "    classified_data = kmeans.labels_\n",
        "    df_processed = yd.copy()\n",
        "    df_processed['Cluster Class'] = pd.Series(classified_data, index=df_processed.index)\n",
        "    y_class = df_processed.iloc[:,-1]\n",
        "    Y_label.append(y_class)\n",
        "\n",
        "    # getting X_ls\n",
        "    X = df.iloc[:, :-1]\n",
        "    X = X._get_numeric_data()\n",
        "    X_ls.append(X)\n",
        "\n",
        "  return(X_ls,Y_ls,Y_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIuKkvFihrL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = glob.glob('/content/drive/My Drive/regression/*.csv')   # create the list of file\n",
        "X_ls, Y_ls, Y_label = get_data(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvzQPdQdtV1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def linear_regression(x_var, y_var):\n",
        "  \"\"\"Applies Linear regression and returns the r-squared score \"\"\"\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x_var, y_var, test_size=0.2, random_state=0)\n",
        "  lr = LinearRegression()  \n",
        "  lr.fit(X_train, y_train)\n",
        "  y_pred = lr.predict(X_test)\n",
        "  score = metrics.r2_score(y_test, y_pred)\n",
        "  return(score)\n",
        "\n",
        "\n",
        "def square(data, feature):\n",
        "  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n",
        "  data_new = data.copy()\n",
        "  transformed_feature = data_new[feature]\n",
        "  transformed_feature = transformed_feature**2\n",
        "  data_new[feature] = transformed_feature\n",
        "  return(data_new)\n",
        "  \n",
        "def freq(data, feature):\n",
        "  \"\"\"convert the feature values to absolute value and then take the frequency\"\"\"\n",
        "  data_new = data.copy()\n",
        "  given_feature = data_new[feature]\n",
        "  frequency = collections.Counter(given_feature) \n",
        "  transformed_feature = [frequency[x] for x in given_feature] \n",
        "  data_new[feature] = transformed_feature\n",
        "  return(data_new)\n",
        "\n",
        "\n",
        "def sigmoid(data, feature):\n",
        "   \"\"\"convert the feature values to absolute value and then take the sigmoid\"\"\"\n",
        "  data_new = data.copy()\n",
        "  transformed_feature = data_new[feature]\n",
        "  transformed_feature = 1/(1 + np.exp(-transformed_feature))\n",
        "  data_new[feature] = transformed_feature\n",
        "  return(data_new)\n",
        "\n",
        "def log(data, feature):\n",
        "  \"\"\"convert the feature values to absolute value and then take the log\"\"\"\n",
        "  data_new = data.copy()\n",
        "  transformed_feature = data_new[feature]\n",
        "  transformed_feature = preprocessing.MinMaxScaler().fit_transform(transformed_feature)\n",
        "  transformed_feature = np.log1p(transformed_feature)\n",
        "  data_new[feature] = transformed_feature\n",
        "  return(data_new)\n",
        "\n",
        "def average(array):\n",
        "  \"\"\"Takes the average\"\"\"\n",
        "  avg = sum(array)/len(array)\n",
        "  return(avg)\n",
        "\n",
        "def scale(data):\n",
        "  \"\"\"Scales the original data\"\"\"\n",
        "  scaled_x = preprocessing.MinMaxScaler().fit_transform(data)\n",
        "  data = pd.DataFrame(data = scaled_x, columns = data.columns)\n",
        "  scaled_data = data.copy()\n",
        "  return(scaled_data)\n",
        "\n",
        "\n",
        "def square_root(data, feature):\n",
        "  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n",
        "  data_new = data.copy()\n",
        "  transformed_feature = data_new[feature]\n",
        "  transformed_feature = abs(transformed_feature)\n",
        "  transformed_feature = np.sqrt(transformed_feature)\n",
        "  data_new[feature] = transformed_feature\n",
        "  return(data_new)\n",
        "\n",
        "def min_max_scale(data):\n",
        "  \"\"\"Scales the data using min-max scaler\"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler = scaler.fit(data)\n",
        "    scaled_value = scaler.transform(data)\n",
        "    return(scaled_value)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3UkjtwBtpAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Automating the feature transformations\n",
        "\n",
        "def feature_trans(y, data, features_list, perc,transformation):\n",
        "  \"\"\"Applies the same transformation for each feature and evaluates the model before and after,\n",
        "  returns a classified training data of positive and negatiive training samples\"\"\"\n",
        "  \n",
        "  pos_samples= []\n",
        "  neg_samples = []\n",
        "  \n",
        "  \n",
        "  original_score = linear_regression(data, y)\n",
        "  \n",
        "  threshold = perc\n",
        "\n",
        "  #threshold = round(((1-original_score)*perc), 3) #setting the threshold as a percentage of the original score\n",
        "  \n",
        "  # Applying Transformation\n",
        "  for col in features_list:\n",
        "    if transformation == 'sqr':  \n",
        "      x = square_root(data, col)\n",
        "    elif transformation == 'square':\n",
        "      x = square(data, col)\n",
        "    elif transformation == 'freq':\n",
        "      x = freq(data, col)\n",
        "    elif transformation == 'log':\n",
        "      x = log(data,col)\n",
        "    elif transformation == 'sig':\n",
        "      x = sigmoid(data, col)\n",
        "    else:\n",
        "      raise RuntimeError('No specified transformation!')\n",
        "\n",
        "    \n",
        "    new_score = linear_regression(x, y)\n",
        "    \n",
        "    if round(new_score, 3) - round(original_score, 3) >= threshold:\n",
        "      pos_samples.append(col)\n",
        "    else:\n",
        "      neg_samples.append(col)\n",
        " # creating positive and negative samples        \n",
        "  positive_samples = [y for x in pos_samples for y in x]\n",
        "  negative_samples = [y for x in neg_samples for y in x]\n",
        "  \n",
        "  return(positive_samples, negative_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNDB3OgdtteQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binning_norm(data, b):\n",
        "    \"\"\"takes in a list of values (data) and number of bins b and bins the values of the feature according \n",
        "  to the bin edges produced by the KBinsDiscretizer function, and returns the normalized value for each bin\"\"\"\n",
        "  model = KBinsDiscretizer(n_bins=b, encode='ordinal', strategy='uniform')\n",
        "\n",
        "  model.fit(data.reshape(-1,1))\n",
        "\n",
        "  bin_edges = model.bin_edges_[0]\n",
        "  bin_values = []\n",
        "  bin_edges = np.delete(bin_edges, 0)\n",
        "  old_ran = 0\n",
        "   #counting number of values in each bin\n",
        "  for x,ran in enumerate(bin_edges):\n",
        "    if x == len(bin_edges)-1:\n",
        "      num = sum(1 for x in data if x <= ran and x >= old_ran)\n",
        "    else:\n",
        "      num = sum(1 for x in data if x < ran and x >= old_ran)\n",
        "    old_ran = ran\n",
        "    bin_values.append(num)\n",
        "  bin_values = [y/len(data) for y in bin_values]\n",
        "  return(bin_values)\n",
        "    \n",
        "#automate QSA\n",
        "#using normalized\n",
        "def binning_normalized(features_list, data, y, class_, bins):\n",
        "   \"\"\"given a list of features, divides the values into two lists accrording to the class,\n",
        "  then scales the values and applies binning norm function, then returns the fixed-size arrays of the data\"\"\"\n",
        "    \n",
        "  bin_edge_pos = []\n",
        "  bin_edge_neg = []\n",
        "  b = {\"bin_edges\":[], \"class\": []}\n",
        "  \n",
        "  for feature in features_list:\n",
        "\n",
        "    data_feature = list(data[feature])\n",
        "    x_f_0 = [z for x,z in enumerate(data_feature) if y[x] == 0]\n",
        "    x_f_1 = [z for x,z in enumerate(data_feature) if y[x] == 1]\n",
        "    x_f_2 = [z for x,z in enumerate(data_feature) if y[x] == 2]\n",
        "    x_f_3 = [z for x,z in enumerate(data_feature) if y[x] == 3]\n",
        "    x_f_4 = [z for x,z in enumerate(data_feature) if y[x] == 4]\n",
        "    x_f_5 = [z for x,z in enumerate(data_feature) if y[x] == 5]\n",
        "    \n",
        "    # reshaping the data\n",
        "    x_f_0 = np.array(x_f_0).reshape(-1,1)\n",
        "    x_f_1 = np.array(x_f_1).reshape(-1,1)\n",
        "    x_f_2 = np.array(x_f_2).reshape(-1,1)\n",
        "    x_f_3 = np.array(x_f_3).reshape(-1,1)\n",
        "    x_f_4 = np.array(x_f_4).reshape(-1,1)\n",
        "    x_f_5 = np.array(x_f_5).reshape(-1,1)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    # scaling the data\n",
        "    x_f_0 = min_max_scale(x_f_0)\n",
        "    x_f_1 = min_max_scale(x_f_1)\n",
        "    x_f_2 = min_max_scale(x_f_2)\n",
        "    x_f_3 = min_max_scale(x_f_3)\n",
        "    x_f_4 = min_max_scale(x_f_4)\n",
        "    x_f_5 = min_max_scale(x_f_5)\n",
        " \n",
        "\n",
        "\n",
        "    # creating training data and labels\n",
        "\n",
        "    bin_all = []\n",
        "    bin_all.extend(binning_norm(x_f_0, bins))\n",
        "    bin_all.extend(binning_norm(x_f_1, bins))\n",
        "    bin_all.extend(binning_norm(x_f_2, bins))\n",
        "    bin_all.extend(binning_norm(x_f_3, bins))\n",
        "    bin_all.extend(binning_norm(x_f_4, bins))\n",
        "    bin_all.extend(binning_norm(x_f_5, bins))\n",
        "  \n",
        "\n",
        "    b[\"bin_edges\"].append(bin_all) \n",
        "    b[\"class\"].append(class_)\n",
        "      \n",
        "  training_data = np.array(b[\"bin_edges\"])\n",
        "  labels = np.array(b[\"class\"])\n",
        "\n",
        "  return(training_data, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rX2fXgIsz4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def make_seq(inc):\n",
        "   \"\"\"creates a sequence of numbers between 0 and 100, and increments it by the specified inc\"\"\"\n",
        "  x = []\n",
        "  start = 0\n",
        "  end = 100\n",
        "\n",
        "  while start < end:\n",
        "    start += inc \n",
        "    start = round(start,2)\n",
        "    x.append(start)\n",
        "  return(x)\n",
        "\n",
        "# Automate Percentile Binning\n",
        "def binning_quantile(features_list, data, y, class_, bins):\n",
        "  \"\"\"creates a feature representation of the features in the feature list using\n",
        "    quantiles by binning all the values lower than a given quantile into one bin, returns\n",
        "    a list of the fixed-size arrays of features along with their label as positive or negative samples\"\"\"\n",
        "    \n",
        "  b = {\"bin_edges\":[], \"class\": []}\n",
        "  quantiles = make_seq(1)\n",
        "\n",
        "  for feature in features_list:\n",
        "\n",
        "    data_feature = list(data[feature])\n",
        "    x_f_0 = [z for x,z in enumerate(data_feature) if y[x] == 0]\n",
        "    x_f_1 = [z for x,z in enumerate(data_feature) if y[x] == 1]\n",
        "    x_f_2 = [z for x,z in enumerate(data_feature) if y[x] == 2]\n",
        "    x_f_3 = [z for x,z in enumerate(data_feature) if y[x] == 3]\n",
        "    x_f_4 = [z for x,z in enumerate(data_feature) if y[x] == 4]\n",
        "    x_f_5 = [z for x,z in enumerate(data_feature) if y[x] == 5]\n",
        "\n",
        "# reshaping the data\n",
        "    x_f_0 = np.array(x_f_0).reshape(-1,1)\n",
        "    x_f_1 = np.array(x_f_1).reshape(-1,1)\n",
        "    x_f_2 = np.array(x_f_2).reshape(-1,1)\n",
        "    x_f_3 = np.array(x_f_3).reshape(-1,1)\n",
        "    x_f_4 = np.array(x_f_4).reshape(-1,1)\n",
        "    x_f_5 = np.array(x_f_5).reshape(-1,1)\n",
        "\n",
        "\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "# scaling the data\n",
        "    x_f_0 = min_max_scale(x_f_0)\n",
        "    x_f_1 = min_max_scale(x_f_1)\n",
        "    x_f_2 = min_max_scale(x_f_2)\n",
        "    x_f_3 = min_max_scale(x_f_3)\n",
        "    x_f_4 = min_max_scale(x_f_4)\n",
        "    x_f_5 = min_max_scale(x_f_5)\n",
        "   \n",
        "\n",
        "    bin_0 = []\n",
        "    bin_1 = []\n",
        "    bin_2 = []\n",
        "    bin_3 = []\n",
        "    bin_4 = []\n",
        "    bin_5 = []\n",
        "\n",
        "\n",
        "    for quant in quantiles:\n",
        "      bin_0.append(np.percentile(x_f_0, quant))\n",
        "      bin_1.append(np.percentile(x_f_1, quant))\n",
        "      bin_2.append(np.percentile(x_f_2, quant))\n",
        "      bin_3.append(np.percentile(x_f_3, quant))\n",
        "      bin_4.append(np.percentile(x_f_4, quant))\n",
        "      bin_5.append(np.percentile(x_f_5, quant)) \n",
        "           \n",
        " # creating training data and labels     \n",
        "    bin_all = []\n",
        "    bin_all.extend(bin_0)\n",
        "    bin_all.extend(bin_1)\n",
        "    bin_all.extend(bin_2)\n",
        "    bin_all.extend(bin_3)\n",
        "    bin_all.extend(bin_4)\n",
        "    bin_all.extend(bin_5)\n",
        "\n",
        "    b[\"bin_edges\"].append(bin_all) \n",
        "    b[\"class\"].append(class_)\n",
        "      \n",
        "  training_data = np.array(b[\"bin_edges\"])\n",
        "  labels = np.array(b[\"class\"])\n",
        "\n",
        "  return(training_data, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pihbtL-ft5xp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discarding datasets with r-squared score greater than 0.97\n",
        "\n",
        "new_x_ls, new_y_ls, new_y_class = [], [], []\n",
        "\n",
        "for x_var, y_var, y_class in zip(X_ls, Y_ls, Y_label):\n",
        "  \n",
        "  scaled_dataset = scale(x_var)\n",
        "  score = linear_regression(x_var, y_var)\n",
        "  if score <= 0.97:\n",
        "    new_x_ls.append(x_var)\n",
        "\n",
        "    new_y_ls.append(y_var)\n",
        "    \n",
        "    new_y_class.append(y_class)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXRqxUFJt8JR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a2a7043-bf97-46b2-ac3d-ab5f1cdf8b3f"
      },
      "source": [
        "import pickle\n",
        "with open('new_x_ls_linr.pickle', 'wb') as handle:\n",
        "    pickle.dump(new_x_ls, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('new_y_ls_linr.pickle', 'wb') as handle:\n",
        "    pickle.dump(new_y_ls, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('new_y_class_linr.pickle', 'wb') as handle:\n",
        "    pickle.dump(new_y_class, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('new_x_ls_linr.pickle') \n",
        "files.download('new_y_ls_linr.pickle')\n",
        "files.download('new_y_class_linr.pickle')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e47cf786-05e5-4a53-9033-33d8fa756694\", \"new_x_ls_linr.pickle\", 31637244)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_abe270b6-f141-4285-b789-04719bd4d6d4\", \"new_y_ls_linr.pickle\", 1464438)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_552f033e-8e4c-4ad5-a7f3-7e2024986c36\", \"new_y_class_linr.pickle\", 737160)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCZuSStqmvzr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0f6316a2-c46c-4f60-eb86-c3b708407412"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UjSMiTvt_HB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "76423b06-262f-4fd2-8609-54dfa7fb9166"
      },
      "source": [
        "# applying log trasformation on positive and negatice samples\n",
        "i = 1\n",
        "pos_list = []\n",
        "neg_list = []\n",
        "\n",
        "for dataset, label in zip(new_x_ls, new_y_ls):\n",
        "  print(\"Training dataset \" + str(i) + \"...\")\n",
        "\n",
        "  scaled_dataset = scale(dataset)\n",
        "\n",
        "  features = [[col] for col in scaled_dataset.columns]\n",
        "\n",
        "  \n",
        "\n",
        "  positive_samples, negative_samples = feature_trans(label, scaled_dataset, features, 0.01,\"log\")\n",
        "\n",
        "  pos_list.append(positive_samples)\n",
        "  neg_list.append(negative_samples)\n",
        "\n",
        "  i +=1 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset 1...\n",
            "Training dataset 2...\n",
            "Training dataset 3...\n",
            "Training dataset 4...\n",
            "Training dataset 5...\n",
            "Training dataset 6...\n",
            "Training dataset 7...\n",
            "Training dataset 8...\n",
            "Training dataset 9...\n",
            "Training dataset 10...\n",
            "Training dataset 11...\n",
            "Training dataset 12...\n",
            "Training dataset 13...\n",
            "Training dataset 14...\n",
            "Training dataset 15...\n",
            "Training dataset 16...\n",
            "Training dataset 17...\n",
            "Training dataset 18...\n",
            "Training dataset 19...\n",
            "Training dataset 20...\n",
            "Training dataset 21...\n",
            "Training dataset 22...\n",
            "Training dataset 23...\n",
            "Training dataset 24...\n",
            "Training dataset 25...\n",
            "Training dataset 26...\n",
            "Training dataset 27...\n",
            "Training dataset 28...\n",
            "Training dataset 29...\n",
            "Training dataset 30...\n",
            "Training dataset 31...\n",
            "Training dataset 32...\n",
            "Training dataset 33...\n",
            "Training dataset 34...\n",
            "Training dataset 35...\n",
            "Training dataset 36...\n",
            "Training dataset 37...\n",
            "Training dataset 38...\n",
            "Training dataset 39...\n",
            "Training dataset 40...\n",
            "Training dataset 41...\n",
            "Training dataset 42...\n",
            "Training dataset 43...\n",
            "Training dataset 44...\n",
            "Training dataset 45...\n",
            "Training dataset 46...\n",
            "Training dataset 47...\n",
            "Training dataset 48...\n",
            "Training dataset 49...\n",
            "Training dataset 50...\n",
            "Training dataset 51...\n",
            "Training dataset 52...\n",
            "Training dataset 53...\n",
            "Training dataset 54...\n",
            "Training dataset 55...\n",
            "Training dataset 56...\n",
            "Training dataset 57...\n",
            "Training dataset 58...\n",
            "Training dataset 59...\n",
            "Training dataset 60...\n",
            "Training dataset 61...\n",
            "Training dataset 62...\n",
            "Training dataset 63...\n",
            "Training dataset 64...\n",
            "Training dataset 65...\n",
            "Training dataset 66...\n",
            "Training dataset 67...\n",
            "Training dataset 68...\n",
            "Training dataset 69...\n",
            "Training dataset 70...\n",
            "Training dataset 71...\n",
            "Training dataset 72...\n",
            "Training dataset 73...\n",
            "Training dataset 74...\n",
            "Training dataset 75...\n",
            "Training dataset 76...\n",
            "Training dataset 77...\n",
            "Training dataset 78...\n",
            "Training dataset 79...\n",
            "Training dataset 80...\n",
            "Training dataset 81...\n",
            "Training dataset 82...\n",
            "Training dataset 83...\n",
            "Training dataset 84...\n",
            "Training dataset 85...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3B7UFt_lZro",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e4183bc6-6f7e-4a15-c6e8-de6b2165c986"
      },
      "source": [
        "# creating training data for MLPs\n",
        "training_data = []\n",
        "training_labels = []\n",
        "i = 1\n",
        "for i, (dataset, label) in enumerate(zip(new_x_ls,new_y_class)):\n",
        "  \n",
        "  print(\"Training dataset \" + str(i) + \"...\")\n",
        "  if len(pos_list[i]) == 0:\n",
        "      neg_training, neg_labels = binning_quantile(neg_list[i], dataset, label, 0,100)\n",
        "      training_data.extend(neg_training)\n",
        "      training_labels.extend(neg_labels)\n",
        "  elif len(neg_list[i]) == 0:\n",
        "    pos_training, pos_labels = binning_quantile(pos_list[i], dataset, label, 1, 100)\n",
        "    training_data.extend(pos_training)\n",
        "    training_labels.extend(pos_labels)\n",
        "  else:\n",
        "    pos_training, pos_labels = binning_quantile(pos_list[i], dataset, label, 1, 100)\n",
        "    neg_training, neg_labels = binning_quantile(neg_list[i], dataset, label, 0, 100)\n",
        "    training_data.extend(pos_training)\n",
        "    training_data.extend(neg_training) \n",
        "    training_labels.extend(pos_labels)\n",
        "    training_labels.extend(neg_labels) \n",
        "  i+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset 0...\n",
            "Training dataset 1...\n",
            "Training dataset 2...\n",
            "Training dataset 3...\n",
            "Training dataset 4...\n",
            "Training dataset 5...\n",
            "Training dataset 6...\n",
            "Training dataset 7...\n",
            "Training dataset 8...\n",
            "Training dataset 9...\n",
            "Training dataset 10...\n",
            "Training dataset 11...\n",
            "Training dataset 12...\n",
            "Training dataset 13...\n",
            "Training dataset 14...\n",
            "Training dataset 15...\n",
            "Training dataset 16...\n",
            "Training dataset 17...\n",
            "Training dataset 18...\n",
            "Training dataset 19...\n",
            "Training dataset 20...\n",
            "Training dataset 21...\n",
            "Training dataset 22...\n",
            "Training dataset 23...\n",
            "Training dataset 24...\n",
            "Training dataset 25...\n",
            "Training dataset 26...\n",
            "Training dataset 27...\n",
            "Training dataset 28...\n",
            "Training dataset 29...\n",
            "Training dataset 30...\n",
            "Training dataset 31...\n",
            "Training dataset 32...\n",
            "Training dataset 33...\n",
            "Training dataset 34...\n",
            "Training dataset 35...\n",
            "Training dataset 36...\n",
            "Training dataset 37...\n",
            "Training dataset 38...\n",
            "Training dataset 39...\n",
            "Training dataset 40...\n",
            "Training dataset 41...\n",
            "Training dataset 42...\n",
            "Training dataset 43...\n",
            "Training dataset 44...\n",
            "Training dataset 45...\n",
            "Training dataset 46...\n",
            "Training dataset 47...\n",
            "Training dataset 48...\n",
            "Training dataset 49...\n",
            "Training dataset 50...\n",
            "Training dataset 51...\n",
            "Training dataset 52...\n",
            "Training dataset 53...\n",
            "Training dataset 54...\n",
            "Training dataset 55...\n",
            "Training dataset 56...\n",
            "Training dataset 57...\n",
            "Training dataset 58...\n",
            "Training dataset 59...\n",
            "Training dataset 60...\n",
            "Training dataset 61...\n",
            "Training dataset 62...\n",
            "Training dataset 63...\n",
            "Training dataset 64...\n",
            "Training dataset 65...\n",
            "Training dataset 66...\n",
            "Training dataset 67...\n",
            "Training dataset 68...\n",
            "Training dataset 69...\n",
            "Training dataset 70...\n",
            "Training dataset 71...\n",
            "Training dataset 72...\n",
            "Training dataset 73...\n",
            "Training dataset 74...\n",
            "Training dataset 75...\n",
            "Training dataset 76...\n",
            "Training dataset 77...\n",
            "Training dataset 78...\n",
            "Training dataset 79...\n",
            "Training dataset 80...\n",
            "Training dataset 81...\n",
            "Training dataset 82...\n",
            "Training dataset 83...\n",
            "Training dataset 84...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PDWh1CcoeEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('train_linears_log01.pickle', 'wb') as handle:\n",
        "    pickle.dump(training_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('label_linears_log01.pickle', 'wb') as handle:\n",
        "    pickle.dump(training_labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download('train_dec_log0.pickle') \n",
        "# files.download('label_dec_log75-2.pickle') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhZn9HjI3JNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}