{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Testing regression_QSA.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"3TUJShYLjqgX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1597251193997,"user_tz":-60,"elapsed":3213,"user":{"displayName":"YUEWEN LI","photoUrl":"","userId":"15636031451830471650"}},"outputId":"11e476a8-b721-4e4f-e63e-a3353b4ee7cd"},"source":["# import libraries\n","from sklearn import preprocessing\n","from sklearn.preprocessing import (LabelEncoder, KBinsDiscretizer, StandardScaler, MinMaxScaler)\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.utils import np_utils\n","from keras.layers.core import Dense, Activation, Dropout\n","from keras.optimizers import Adam, Nadam\n","from sklearn.utils import class_weight\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","from keras.callbacks import ModelCheckpoint\n","import imblearn\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import BorderlineSMOTE\n","from sklearn.metrics import confusion_matrix\n","import sklearn\n","import tensorflow as tf\n","from google.colab import files\n","import pickle\n","from sklearn.linear_model import (LogisticRegression, LogisticRegressionCV)\n","from sklearn.datasets import make_classification\n","from sklearn.ensemble import (GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor)\n","from imblearn.over_sampling import SMOTE\n","import collections\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import cross_val_score\n","from pandas import DataFrame\n","from sklearn.cluster import KMeans\n","from sklearn.linear_model import LinearRegression\n","from sklearn import metrics\n","\n","import warnings\n","import sys\n","\n","if not sys.warnoptions:\n","    warnings.simplefilter(\"ignore\")\n","\n","warnings.simplefilter(action='ignore', category=FutureWarning)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Me4Vt8rjoYoO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1597251290961,"user_tz":-60,"elapsed":18693,"user":{"displayName":"YUEWEN LI","photoUrl":"","userId":"15636031451830471650"}},"outputId":"511fb518-bcc2-4019-ca99-dfd67fa0a40e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XxDRUXAaxNgA","colab_type":"code","colab":{}},"source":["# data preprocessing - using K-means to cluster data points and label them accordingly\n","def get_data(list_of_files):\n","  X_ls = []\n","  Y_ls = []\n","  Y_label = []\n","  for filename in list_of_files:\n","    if filename[-1] == 'x':\n","      df = pd.read_excel(filename)\n","    else:\n","      df = pd.read_csv(filename)\n","    df = df.dropna()\n","\n","    Y = df.iloc[:,-1]\n","    Y = np.array(Y).reshape(-1, 1)\n","    scaler = MinMaxScaler()\n","    Y = pd.DataFrame(scaler.fit_transform(Y))\n","    Y_ls.append(Y)\n","    \n","    # getting Y_label\n","    yd = DataFrame(Y)\n","    y = yd.values\n","    kmeans = KMeans(n_clusters=6).fit(y)\n","    classified_data = kmeans.labels_\n","    df_processed = yd.copy()\n","    df_processed['Cluster Class'] = pd.Series(classified_data, index=df_processed.index)\n","    y_class = df_processed.iloc[:,-1]\n","    Y_label.append(y_class)\n","\n","    # getting X_ls\n","    X = df.iloc[:, :-1]\n","    X = X._get_numeric_data()\n","    X_ls.append(X)\n","\n","  return(X_ls,Y_ls,Y_label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xnPp_080wBID","colab_type":"code","colab":{}},"source":["import glob\n","test_files = glob.glob('/content/drive/My Drive/TESTING DATASETS/reg_testing_datasets/*.csv')\n","test_files_xlsx = glob.glob('/content/drive/My Drive/TESTING DATASETS/reg_testing_datasets/*.xlsx')\n","test_files.extend(test_files_xlsx)   # create the list of file\n","\n","X_ls, Y_ls, Y_label = get_data(test_files)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSLf0ls7yXLE","colab_type":"code","colab":{}},"source":["# prepare test data\n","\n","def scale(X, y):\n","  '''\n","  scale the test data\n","  '''\n","  scaled_X = preprocessing.MinMaxScaler().fit_transform(X)\n","  X = pd.DataFrame(data = scaled_X, columns = X.columns)\n","\n","  scaled_data = X.copy()\n","  scaled_data['class'] = y\n","\n","  return X, scaled_data\n","\n","# making sequence like seq(start, end, increment) in R\n","def make_seq(inc):\n","  x = []\n","  start = 0\n","  end = 100\n","\n","  while start < end:\n","    start += inc \n","    start = round(start,2)\n","    x.append(start)\n","  return(x)\n","\n","def percentiles(x, b):\n","  # return np.percentile(x, np.linspace(0,100,num = b))\n","  c = 100/b\n","  return np.percentile(x, make_seq(c))\n","\n","def qsa(x, b):\n","  '''\n","  return a list of length b within range (lb, ub) representing feature x\n","  '''\n","  width = 20 / b\n","  x = np.sort(x)\n","  res = [0] * b\n","  for i in x:\n","    num = int((i - -10) // width)\n","    if num == b:\n","      res[b-1] += 1\n","    else:\n","      res[num] += 1\n","  res = [i / len(x) for i in res]\n","  return res\n","\n","def bin_edge(X, scaled_data, lb, ub, b, qsa_):\n","  '''\n","  return a representation of features in a dataset\n","  '''\n","  col_names = X.columns\n","\n","  cls0 = scaled_data[scaled_data['class'] == 0]\n","  cls1 = scaled_data[scaled_data['class'] == 1]\n","  cls2 = scaled_data[scaled_data['class'] == 2]\n","  cls3 = scaled_data[scaled_data['class'] == 3]\n","  cls4 = scaled_data[scaled_data['class'] == 4]\n","  cls5 = scaled_data[scaled_data['class'] == 5]\n","  \n","  #scale to range(-10,10)\n","  cls0_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(cls0[col_names])\n","  cls0_scaler = pd.DataFrame(data = cls0_scaler, columns = col_names)\n","  cls1_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(cls1[col_names])\n","  cls1_scaler = pd.DataFrame(data = cls1_scaler, columns = col_names)\n","  cls2_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(cls2[col_names])\n","  cls2_scaler = pd.DataFrame(data = cls2_scaler, columns = col_names)\n","  cls3_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(cls3[col_names])\n","  cls3_scaler = pd.DataFrame(data = cls3_scaler, columns = col_names)\n","  cls4_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(cls4[col_names])\n","  cls4_scaler = pd.DataFrame(data = cls4_scaler, columns = col_names)\n","  cls5_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(cls5[col_names])\n","  cls5_scaler = pd.DataFrame(data = cls5_scaler, columns = col_names)\n","\n","  training_data = []\n","  \n","  for col in col_names:\n","\n","    bin_all = []\n","\n","    if qsa_ == True:\n","\n","      bin_all.extend(qsa(cls0_scaler[col], b))\n","      bin_all.extend(qsa(cls1_scaler[col], b))\n","      bin_all.extend(qsa(cls2_scaler[col], b))\n","      bin_all.extend(qsa(cls3_scaler[col], b))\n","      bin_all.extend(qsa(cls4_scaler[col], b))\n","      bin_all.extend(qsa(cls5_scaler[col], b))\n","\n","    else:\n","      bin_all.extend(percentiles(cls0_scaler[col], b))\n","      bin_all.extend(percentiles(cls1_scaler[col], b))\n","      bin_all.extend(percentiles(cls2_scaler[col], b))\n","      bin_all.extend(percentiles(cls3_scaler[col], b))\n","      bin_all.extend(percentiles(cls4_scaler[col], b))\n","      bin_all.extend(percentiles(cls5_scaler[col], b))\n","    \n","    training_data.append(bin_all)\n","\n","  training_data = np.array(training_data)\n","\n","  return training_data\n","\n","def min_max_scale(data):\n","    scaler = MinMaxScaler()\n","    scaler = scaler.fit(data)\n","    scaled_value = scaler.transform(data)\n","    return(scaled_value)\n","\n","def create_testdata(X, Y, bins, qsa_):\n","  '''\n","  return test data to be fed into mlps\n","  '''\n","  test_data = []\n","  \n","  X, scaled_data = scale(X, Y)\n","  test = bin_edge(X, scaled_data, -10, 10, bins, qsa_)\n","  test_data.extend(test)\n","  \n","  return test_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWyi6xClTTks","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1597223567728,"user_tz":-60,"elapsed":499,"user":{"displayName":"YUEWEN LI","photoUrl":"","userId":"15636031451830471650"}},"outputId":"6a4103e2-d3b4-4b70-db9f-280b10a9df57"},"source":["# test the computational time on a random dataset\n","X, scaled_data = scale(X_ls[0],Y_label[0])\n","\n","start = time.time()\n","b = bin_edge(X, scaled_data, -10, 10, 100, False)\n","end = time.time()\n","print(end - start)\n","print(b[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.04091215133666992\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["600"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"en8UQ6rrHrmS","colab_type":"text"},"source":["# Output recommendation results"]},{"cell_type":"markdown","metadata":{"id":"u4TTvpghysC4","colab_type":"text"},"source":["### output all transformations per dataset"]},{"cell_type":"code","metadata":{"id":"GejD7UzYlwSU","colab_type":"code","colab":{}},"source":["# recommend all\n","def recommend(path, b, X, Y, qsa_, threshold):\n","  # iterate over all models\n","  models_ls = glob.glob(path)\n","\n","  # res = []\n","\n","  test_new = X.copy()\n","  # for one test data\n","  test_data = create_testdata(X, Y, b, qsa_)\n","\n","  colnames = X.columns\n","  \n","  row = []\n","  scores = []\n","\n","  for m in models_ls:\n","    row.append(m.split('/')[-1][:-3])\n","    model = tf.python.keras.models.load_model(m)\n","    pred = model.predict(np.array(test_data), verbose=1)\n","    # the score for positive label\n","    scores.append(pred[:,1])\n","\n","  result = pd.DataFrame(scores, columns = colnames, index = row)\n","  \n","  # print(result.head())\n","  # print('Recommended transformation for ',test_files[i].split('/')[-1][:-4])\n","  # print('\\n')\n","\n","  dict_ = {}\n","\n","  cnt = 0\n","  \n","  for col in colnames:\n","    sorted_result = result.sort_values(axis = 0, by = col, ascending = False)\n","    rcm_trans = sorted_result[sorted_result[col] >= threshold].index.tolist()\n","    dict_[col] = rcm_trans\n","    print(col+': ',rcm_trans)\n","    if len(rcm_trans) > 0:\n","      cnt += 1\n","\n","  # if cnt == 0:\n","  #   print('No transformation is recommended for this dataset!!!')\n","    \n","  # res.append(dict_)\n","  \n","  # print('\\n')\n","\n","  return dict_, cnt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dt7ruIxAGAge","colab_type":"code","colab":{}},"source":["# classifiers\n","def linear_regression(x_var, y_var):\n","  X_train, X_test, y_train, y_test = train_test_split(x_var, y_var, test_size=0.2, random_state=0)\n","  lr = LinearRegression()  \n","  lr.fit(X_train, y_train)\n","  y_pred = lr.predict(X_test)\n","  score = metrics.r2_score(y_test, y_pred)\n","  return(score)\n","\n","def decision_tree(x_var, y_var):\n","  X_train, X_test, y_train, y_test = train_test_split(x_var, y_var, test_size=0.2, random_state=0)\n","  clf = tree.DecisionTreeRegressor(random_state = 0)\n","  clf.fit(X_train,y_train)\n","  y_pred = clf.predict(X_test)\n","  score = metrics.r2_score(y_test, y_pred)\n","  return(score)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SSpzjRtixWUD","colab_type":"text"},"source":["## Replace features"]},{"cell_type":"code","metadata":{"id":"vBs_kywzVn6F","colab_type":"code","colab":{}},"source":["# transformations\n","def freq(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  given_feature = data_new[feature]\n","  frequency = collections.Counter(given_feature) \n","  transformed_feature = [frequency[x] for x in given_feature] \n","  data_new[feature] = transformed_feature\n","  return(data_new)\n","\n","def sq(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = transformed_feature**2\n","  data_new[feature] = transformed_feature\n","  return(data_new)\n","\n","def sqr(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = abs(transformed_feature)\n","  transformed_feature = np.sqrt(transformed_feature)\n","  data_new[feature] = transformed_feature\n","  return(data_new)\n","\n","def log(data, feature):\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = abs(transformed_feature)\n","  transformed_feature = np.where(transformed_feature == 0, 1e-9, transformed_feature)\n","  transformed_feature = np.log(transformed_feature)\n","  data_new[feature] = transformed_feature\n","  return(data_new)\n","\n","def sigmoid(data, feature):\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = 1/(1 + np.exp(-transformed_feature))\n","  data_new[feature] = transformed_feature\n","  return(data_new)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Z4C5exsxaQK","colab_type":"text"},"source":["## Add features"]},{"cell_type":"code","metadata":{"id":"7WG1Kl_exU_J","colab_type":"code","colab":{}},"source":["# transformations\n","def freq_add(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  given_feature = data_new[feature]\n","  frequency = collections.Counter(given_feature) \n","  transformed_feature = [frequency[x] for x in given_feature] \n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)\n","\n","def sq_add(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = transformed_feature**2\n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)\n","\n","def sqr_add(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = abs(transformed_feature)\n","  transformed_feature = np.sqrt(transformed_feature)\n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)\n","\n","def log_add(data, feature):\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = abs(transformed_feature)\n","  transformed_feature = np.where(transformed_feature == 0, 1e-9, transformed_feature)\n","  transformed_feature = np.log(transformed_feature)\n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)\n","\n","def sigmoid_add(data, feature):\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = 1/(1 + np.exp(-transformed_feature))\n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CdqPxJZ8H7nE","colab_type":"text"},"source":["# Apply transformations"]},{"cell_type":"code","metadata":{"id":"eqJr4DBewDvP","colab_type":"code","colab":{}},"source":["def apply_trans(dict_, X, Y, add):\n","\n","  score_before_ls = []\n","  score_after_ls = []\n","  diff_ls = []\n","  file_ls = []\n","  \n","  test_original = X.copy()\n","  label = Y\n","\n","  test_original, scaled_data = scale(test_original, label)\n","\n","  colnames = X.columns\n","\n","  # apply transformation\n","  if add == True:\n","    methods = {'sq': sq_add, 'sqr':sqr_add, 'freq':freq_add, 'log':log_add, 'sig': sigmoid_add}\n","  else:\n","    methods = {'sq': sq, 'sqr':sqr, 'freq':freq, 'log':log, 'sig': sigmoid}\n","  test_new = test_original.copy()\n","\n","  # dict_ = res[i]\n","  \n","  '''\n","  for col in colnames:\n","    if len(dict_[col]) > 0:\n","      for i in dict_[col]:\n","        test_new = methods[i](test_new, col)\n","      test_new.drop(columns=[col],inplace = True)\n","  '''\n","  for col in colnames:\n","    if len(dict_[col]) > 0:\n","      test_new = methods[dict_[col][0]](test_new, col)  \n","  \n","\n","  '''\n","  # corresponding to one transformation per dataset\n","  col = list(dict_.keys())[0]\n","  test_new = methods[dict_[col]](test_new, col)\n","  '''\n","\n","  return test_new, label\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpCI3Rw9HoV-","colab_type":"code","colab":{}},"source":["def eval(avg, file_name, test_original, test_new, label, *args):\n","\n","  if avg == True:\n","    score_lnr0 = linear_regression(test_original, label)\n","    score_dtr0 = decision_tree(test_original, label)\n","    \n","    score_before = (score_lnr0 + score_dtr0) / 2\n","\n","    score_lnr1 = linear_regression(test_new, label)\n","    score_dtr1 = decision_tree(test_new, label)\n","    \n","    score_after = (score_lnr1 + score_dtr1) / 2\n","\n","  else:\n","    model = args[0]\n","    if model == 'lnr':\n","      score_before = linear_regression(test_original, label)\n","      score_after = linear_regression(test_new, label)\n","\n","    elif model == 'dtr':\n","      score_before = decision_tree(test_original, label)\n","      score_after = decision_tree(test_new, label)\n","  \n","  if score_after > score_before:\n","    improve = ' improved'\n","  else:\n","    improve = ' not improved'\n","\n","  diff = score_after - score_before\n","\n","  print(file_name, ' original score: ',score_before,'; score after: ',score_after, '; difference: ', diff ,improve)\n","  # print(test_files[i].split('/')[-1][:-4], improve)\n","\n","  return score_before, score_after, diff, file_name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ch9UHBq1G0EW","colab_type":"code","colab":{}},"source":["# combanation of trasformations\n","def multiple_testing(path, b, n, add, model, qsa_, threshold):\n","  score_before_ls = []\n","  score_after_ls = []\n","  diff_ls = []\n","  file_ls = []\n","\n","  for i in range(len(test_files)):\n","    X = X_ls[i].copy()\n","    Y = Y_ls[i].copy()\n","    label = Y_label[i].copy()\n","    X_origin, scaled_origin = scale(X,Y)\n","    for j in range(n):\n","      res, cnt = recommend(path, b, X, label, qsa_, threshold)\n","      if cnt > 0:\n","        X_new, Y = apply_trans(res, X, Y, add)\n","        X = X_new\n","    score_before, score_after, diff, file = eval(False, test_files[i].split('/')[-1][:-4], X_origin, X, Y, model)\n","    # if score_before <= 0.999 and score_before > 0:\n","    score_before_ls.append(score_before)\n","    score_after_ls.append(score_after)\n","    diff_ls.append(score_after - score_before)\n","    file_ls.append(test_files[i].split('/')[-1][:-4])\n","\n","  return score_before_ls, score_after_ls, diff_ls, file_ls\n"],"execution_count":null,"outputs":[]}]}