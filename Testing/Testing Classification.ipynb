{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Testing.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"3TUJShYLjqgX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1597248605709,"user_tz":-60,"elapsed":3620,"user":{"displayName":"YUEWEN LI","photoUrl":"","userId":"15636031451830471650"}},"outputId":"a9ab4899-9d9f-4460-a073-37ac1ff403ce"},"source":["# import libraries\n","from sklearn import preprocessing\n","from sklearn.preprocessing import (LabelEncoder, KBinsDiscretizer, StandardScaler, MinMaxScaler)\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.utils import np_utils\n","from keras.layers.core import Dense, Activation, Dropout\n","from keras.optimizers import Adam, Nadam\n","from sklearn.utils import class_weight\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","from keras.callbacks import ModelCheckpoint\n","import imblearn\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import BorderlineSMOTE\n","from sklearn.metrics import confusion_matrix\n","import sklearn\n","import tensorflow as tf\n","from google.colab import files\n","import pickle\n","from sklearn.linear_model import (LogisticRegression, LogisticRegressionCV)\n","from sklearn.datasets import make_classification\n","from sklearn.ensemble import (GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor)\n","from imblearn.over_sampling import SMOTE\n","import collections\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import cross_val_score\n","from imblearn.pipeline import Pipeline, make_pipeline\n","\n","import warnings\n","import sys\n","\n","if not sys.warnoptions:\n","    warnings.simplefilter(\"ignore\")\n","\n","warnings.simplefilter(action='ignore', category=FutureWarning)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Me4Vt8rjoYoO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1597248638116,"user_tz":-60,"elapsed":26487,"user":{"displayName":"YUEWEN LI","photoUrl":"","userId":"15636031451830471650"}},"outputId":"aacba2a8-e552-4fba-f2ad-ccae60433275"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XxDRUXAaxNgA","colab_type":"code","colab":{}},"source":["# define a function to extract response variable from a dataset\n","def labelencode(data):\n","    labelencoder = LabelEncoder()\n","    #Assigning numerical values and storing in another column\n","    Y = data.iloc[:,-1]\n","    Y = labelencoder.fit_transform(Y)\n","    return Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xnPp_080wBID","colab_type":"code","colab":{}},"source":["import glob\n","test_files = glob.glob('/content/drive/My Drive/TESTING DATASETS/clas_testing_datasets/*.csv')   # create the list of file\n","\n","test_X_ls = []\n","test_Y_ls = []\n","for filename in test_files:\n","    df = pd.read_csv(filename)\n","    Y = labelencode(df)\n","    X = df.iloc[:,:-1]\n","    X = X._get_numeric_data()\n","    test_Y_ls.append(Y)\n","    test_X_ls.append(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSLf0ls7yXLE","colab_type":"code","colab":{}},"source":["# prepare test data for MLP\n","\n","def scale(X, y):\n","  '''\n","  scale the test data\n","  '''\n","  scaled_X = preprocessing.MinMaxScaler().fit_transform(X)\n","  X = pd.DataFrame(data = scaled_X, columns = X.columns)\n","\n","  scaled_data = X.copy()\n","  scaled_data['class'] = y\n","\n","  return X, scaled_data\n","\n","# making sequence like seq(start, end, increment) in R\n","def make_seq(inc):\n","  x = []\n","  start = 0\n","  end = 100\n","\n","  while start < end:\n","    start += inc \n","    start = round(start,2)\n","    x.append(start)\n","  return(x)\n","\n","def percentiles(x, b):\n","  # return np.percentile(x, np.linspace(0,100,num = b))\n","  c = 100/b\n","  return np.percentile(x, make_seq(c))\n","\n","# creating a quantile sketch array of x with lower bound lb, upper bound ub and number of bins b\n","def qsa(x, lb, ub, b):\n","  '''\n","  return a list of length b within range (lb, ub) representing feature x\n","  '''\n","  width = (ub - lb) / b\n","  x = np.sort(x)\n","  res = [0] * b\n","  for i in x:\n","    num = int((i - lb) // width)\n","    if num == b:\n","      res[b-1] += 1\n","    else:\n","      res[num] += 1\n","  res = [i / len(x) for i in res]\n","  return res\n","\n","def bin_edge(X, scaled_data, lb, ub, b, qsa_):\n","  '''\n","  return a representation of features in a dataset X\n","  if qsa_ is True, use QSA\n","  otherwise use PB\n","  '''\n","  col_names = X.columns\n","\n","  pos = scaled_data[scaled_data['class'] == 1]\n","  neg = scaled_data[scaled_data['class'] == 0]\n","  \n","  #scale to range(-10,10)\n","  pos_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(pos[col_names])\n","  pos_scaler = pd.DataFrame(data = pos_scaler, columns = col_names)\n","  neg_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(neg[col_names])\n","  neg_scaler = pd.DataFrame(data = neg_scaler, columns = col_names)\n","\n","  training_data = []\n","\n","  for col in col_names:\n","\n","    bin_all = []\n","\n","    if qsa_ == True:\n","      bin_all.extend(qsa(pos_scaler[col], lb, ub, b))\n","      bin_all.extend(qsa(neg_scaler[col], lb, ub, b))\n","\n","    else:\n","      # quantile\n","      bin_all.extend(percentiles(pos_scaler[col], b))\n","      bin_all.extend(percentiles(neg_scaler[col], b))\n","\n","    training_data.append(bin_all)\n","\n","  training_data = np.array(training_data)\n","\n","  return training_data\n","\n","def create_testdata(X, Y, bins, qsa_):\n","  '''\n","  return test data to be fed into mlps\n","  '''\n","  test_data = []\n","  \n","  X, scaled_data = scale(X, Y)\n","  features_list = X.columns\n","  test = bin_edge(X, scaled_data, -10, 10, bins, qsa_)\n","  test_data.extend(test)\n","  \n","  return test_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cXDt8qKbdS28","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"status":"ok","timestamp":1597221760236,"user_tz":-60,"elapsed":737,"user":{"displayName":"YUEWEN LI","photoUrl":"","userId":"15636031451830471650"}},"outputId":"40b50985-e581-4825-ba41-903f2a3a82dd"},"source":["# test the computational time on a random dataset\n","X, scaled_data = scale(test_X_ls[0],test_Y_ls[0])\n","\n","start = time.time()\n","b = bin_edge(X, scaled_data, -10, 10, 200, False)\n","end = time.time()\n","print(end - start)\n","b[0][:20]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.05429530143737793\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([-9.40608439, -8.52631684, -7.99658281, -7.59879802, -7.30916135,\n","       -6.81337075, -6.681948  , -6.56119403, -6.20528969, -5.8552661 ,\n","       -4.70486503, -3.89342141, -3.65630417, -3.32123078, -3.15494077,\n","       -3.08335858, -3.06509052, -2.86653587, -2.51287655, -2.43648645])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"en8UQ6rrHrmS","colab_type":"text"},"source":["# Output recommendation results"]},{"cell_type":"markdown","metadata":{"id":"u4TTvpghysC4","colab_type":"text"},"source":["### output all transformations per dataset"]},{"cell_type":"code","metadata":{"id":"GejD7UzYlwSU","colab_type":"code","colab":{}},"source":["# recommend all\n","def recommend(path, b, X, Y, qsa_, threshold):\n","  # iterate over all models\n","  models_ls = glob.glob(path)\n","\n","  # res = []\n","\n","  test_new = X.copy()\n","  # for one test data\n","  test_data = create_testdata(X, Y, b, qsa_)\n","\n","  colnames = X.columns\n","  \n","  row = []\n","  scores = []\n","\n","  for m in models_ls:\n","    row.append(m.split('/')[-1][:-3])\n","    model = tf.python.keras.models.load_model(m)\n","    pred = model.predict(np.array(test_data), verbose=1)\n","    # the score for positive label\n","    scores.append(pred[:,1])\n","\n","  result = pd.DataFrame(scores, columns = colnames, index = row)\n","  \n","  # print(result.head())\n","  # print('Recommended transformation for ',test_files[i].split('/')[-1][:-4])\n","  # print('\\n')\n","\n","  dict_ = {}\n","\n","  cnt = 0\n","  \n","  for col in colnames:\n","    sorted_result = result.sort_values(axis = 0, by = col, ascending = False)\n","    rcm_trans = sorted_result[sorted_result[col] >= threshold].index.tolist()\n","    dict_[col] = rcm_trans\n","    print(col+': ',rcm_trans)\n","    if len(rcm_trans) > 0:\n","      cnt += 1\n","\n","  # if cnt == 0:\n","  #   print('No transformation is recommended for this dataset!!!')\n","    \n","  # res.append(dict_)\n","  \n","  # print('\\n')\n","\n","  return dict_, cnt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dt7ruIxAGAge","colab_type":"code","colab":{}},"source":["# classifiers\n","def lr_smote(x_train, y_train):\n","  imba_pipeline = make_pipeline(SMOTE(random_state=42), LogisticRegression(max_iter=500, solver=\"liblinear\", random_state=0))\n","  score = cross_val_score(imba_pipeline, x_train, y_train, scoring='f1', cv=5)\n","  score = np.mean(score)\n","  return(score) \n","def random_forest(data, labels):\n","  rfc = RandomForestClassifier(max_depth=3, random_state=0)\n","  score = cross_val_score(rfc, data, labels, cv=5, scoring='f1')\n","  score = np.mean(score)\n","  return(score)\n","  \n","def rf_smote(x_train, y_train):\n","  imba_pipeline = make_pipeline(SMOTE(random_state=42), \n","                                RandomForestClassifier(max_depth=3, random_state=0))\n","  score = cross_val_score(imba_pipeline, x_train, y_train, scoring='f1', cv=5)\n","  score = np.mean(score)\n","  return(score) \n","  \n","def log_reg(max_iter, X, Y):\n","  \"\"\"Applies logistic regression given x features and vector y of labels \"\"\"\n","  logisticRegr = LogisticRegression(max_iter=max_iter, solver=\"liblinear\", random_state=0)\n","  score = cross_val_score(logisticRegr, X, Y, cv=5, scoring='f1')\n","  score = np.mean(score)\n","  return(score)\n","\n","def smote(X,Y):\n","  \"\"\"Applies smote to fix class imbalance\"\"\"\n","  smt = SMOTE()\n","  x_train, y_train = smt.fit_sample(X, Y)\n","  x = pd.DataFrame(data = x_train, columns = X.columns)\n","  return(x, y_train)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SSpzjRtixWUD","colab_type":"text"},"source":["## Replace features"]},{"cell_type":"code","metadata":{"id":"vBs_kywzVn6F","colab_type":"code","colab":{}},"source":["# transformations\n","def freq(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  given_feature = data_new[feature]\n","  frequency = collections.Counter(given_feature) \n","  transformed_feature = [frequency[x] for x in given_feature] \n","  data_new[feature] = transformed_feature\n","  return(data_new)\n","\n","def sq(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = transformed_feature**2\n","  data_new[feature] = transformed_feature\n","  return(data_new)\n","\n","def sqr(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = abs(transformed_feature)\n","  transformed_feature = np.sqrt(transformed_feature)\n","  data_new[feature] = transformed_feature\n","  return(data_new)\n","\n","def log(data, feature):\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = abs(transformed_feature)\n","  transformed_feature = np.where(transformed_feature == 0, 1e-9, transformed_feature)\n","  transformed_feature = np.log(transformed_feature)\n","  data_new[feature] = transformed_feature\n","  return(data_new)\n","\n","def sigmoid(data, feature):\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = 1/(1 + np.exp(-transformed_feature))\n","  data_new[feature] = transformed_feature\n","  return(data_new)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Z4C5exsxaQK","colab_type":"text"},"source":["## Add features"]},{"cell_type":"code","metadata":{"id":"7WG1Kl_exU_J","colab_type":"code","colab":{}},"source":["# transformations\n","def freq_add(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  given_feature = data_new[feature]\n","  frequency = collections.Counter(given_feature) \n","  transformed_feature = [frequency[x] for x in given_feature] \n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)\n","\n","def sq_add(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = transformed_feature**2\n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)\n","\n","def sqr_add(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = abs(transformed_feature)\n","  transformed_feature = np.sqrt(transformed_feature)\n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)\n","\n","def log_add(data, feature):\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = abs(transformed_feature)\n","  transformed_feature = np.where(transformed_feature == 0, 1e-9, transformed_feature)\n","  transformed_feature = np.log(transformed_feature)\n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)\n","\n","def sigmoid_add(data, feature):\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = 1/(1 + np.exp(-transformed_feature))\n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CdqPxJZ8H7nE","colab_type":"text"},"source":["# Apply transformations"]},{"cell_type":"code","metadata":{"id":"eqJr4DBewDvP","colab_type":"code","colab":{}},"source":["def apply_trans(dict_, X, Y, add):\n","\n","  score_before_ls = []\n","  score_after_ls = []\n","  diff_ls = []\n","  file_ls = []\n","  \n","  test_original = X.copy()\n","  label = Y\n","\n","  test_original, scaled_data = scale(test_original, label)\n","\n","  colnames = X.columns\n","\n","  # apply transformation\n","  if add == True:\n","    methods = {'sq': sq_add, 'sqr':sqr_add, 'freq':freq_add, 'log':log_add, 'sig': sigmoid_add}\n","  else:\n","    methods = {'sq': sq, 'sqr':sqr, 'freq':freq, 'log':log, 'sig': sigmoid}\n","  test_new = test_original.copy()\n","\n","  # dict_ = res[i]\n","  \n","  '''\n","  for col in colnames:\n","    if len(dict_[col]) > 0:\n","      for i in dict_[col]:\n","        test_new = methods[i](test_new, col)\n","      test_new.drop(columns=[col],inplace = True)\n","  '''\n","  for col in colnames:\n","    if len(dict_[col]) > 0:\n","      test_new = methods[dict_[col][0]](test_new, col)  \n","  \n","\n","  '''\n","  # corresponding to one transformation per dataset\n","  col = list(dict_.keys())[0]\n","  test_new = methods[dict_[col]](test_new, col)\n","  '''\n","\n","  return test_new, label\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mh4BvPgpW9yr","colab_type":"text"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"id":"MpCI3Rw9HoV-","colab_type":"code","colab":{}},"source":["def eval(avg, file_name, test_original, test_new, label, *args):\n","\n","  if avg == True:\n","    # take the average score of multiple classifiers\n","    score_log_reg0 = log_reg(500, test_original, label)\n","    score_rf0 = random_forest(test_original, label, 5)\n","    score_gb0 = gb(test_original, label, 0.04)\n","\n","    score_before = (score_log_reg0 + score_rf0 + score_gb0) / 3\n","\n","    score_log_reg1 = log_reg(500, test_new, label)\n","    score_rf1 = random_forest(test_new, label, 5)\n","    score_gb1 = gb(test_new, label, 0.04)\n","\n","    score_after = (score_log_reg1 + score_rf1 + score_gb1) / 3\n","\n","  else:\n","    model = args[0]\n","\n","    count_0 = len([1 for y in label if y == 0])\n","    count_1 = len([1 for y in label if y == 1])\n","    min_count = min(count_0,count_1)\n","    \n","\n","    if model == 'lr':\n","      if min_count/len(label) < 0.4: \n","        score_before = lr_smote(test_original, label)\n","        score_after = lr_smote(test_new, label)\n","      else:\n","        score_before = log_reg(500, test_original, label)\n","        score_after = log_reg(500, test_new, label)\n","\n","    elif model == 'rf':\n","      if min_count/len(label) < 0.4: \n","        score_before = rf_smote(test_original, label)\n","        score_after = rf_smote(test_new, label)\n","      else:\n","        score_before = random_forest(test_original, label)\n","        score_after = random_forest(test_new, label)\n","\n","    elif model == 'gb':\n","      score_before = gb(test_original, label, 0.02)\n","      score_after = gb(test_new, label, 0.02)\n","\n","  \n","  if score_after > score_before:\n","    improve = ' improved'\n","  else:\n","    improve = ' not improved'\n","\n","  diff = score_after - score_before\n","\n","  print(file_name, ' original score: ',score_before,'; score after: ',score_after, '; difference: ', diff ,improve)\n","  # print(test_files[i].split('/')[-1][:-4], improve)\n","\n","  return score_before, score_after, diff, file_name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ch9UHBq1G0EW","colab_type":"code","colab":{}},"source":["# combanation of trasformations\n","def multiple_testing(path, b, n, add, model, qsa_, threshold):\n","  score_before_ls = []\n","  score_after_ls = []\n","  diff_ls = []\n","  file_ls = []\n","\n","  for i in range(len(test_files)):\n","    X = test_X_ls[i].copy()\n","    Y = test_Y_ls[i].copy()\n","    # update on 30/7/2020 add scaled original X\n","    X_original, X_scaled = scale(X,Y)\n","    \n","    # updatedon 8/8/2020 use unscaled dataset to get original score\n","    \n","    for j in range(n):\n","      res, cnt = recommend(path, b, X, Y, qsa_, threshold)\n","      if cnt > 0:\n","        X_new, label = apply_trans(res, X, Y, add)\n","        X = X_new\n","      else:\n","        X = X_original\n","    score_before, score_after, diff, file = eval(False, test_files[i].split('/')[-1][:-4], X_original, X, test_Y_ls[i], model)\n","    # if score_before <= 0.999 and score_before > 0:\n","    score_before_ls.append(score_before)\n","    score_after_ls.append(score_after)\n","    diff_ls.append(score_after - score_before)\n","    file_ls.append(test_files[i].split('/')[-1][:-4])\n","\n","  return score_before_ls, score_after_ls, diff_ls, file_ls\n"],"execution_count":null,"outputs":[]}]}