{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Testing binary.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"3TUJShYLjqgX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1597358942604,"user_tz":-60,"elapsed":3016,"user":{"displayName":"YUEWEN LI","photoUrl":"","userId":"15636031451830471650"}},"outputId":"e6c58eed-d023-4cae-9e21-30145a0a0f26"},"source":["from sklearn import preprocessing\n","from sklearn.preprocessing import (LabelEncoder, KBinsDiscretizer, StandardScaler, MinMaxScaler)\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.utils import np_utils\n","from keras.layers.core import Dense, Activation, Dropout\n","from keras.optimizers import Adam, Nadam\n","from sklearn.utils import class_weight\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","from keras.callbacks import ModelCheckpoint\n","import imblearn\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import BorderlineSMOTE\n","from sklearn.metrics import confusion_matrix\n","import sklearn\n","import tensorflow as tf\n","from google.colab import files\n","import pickle\n","from sklearn.linear_model import (LogisticRegression, LogisticRegressionCV)\n","from sklearn.datasets import make_classification\n","from sklearn.ensemble import (GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor)\n","from imblearn.over_sampling import SMOTE\n","import collections\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import cross_val_score\n","import itertools\n","import random\n","from random import sample\n","from imblearn.pipeline import Pipeline, make_pipeline\n","\n","import warnings\n","import sys\n","\n","if not sys.warnoptions:\n","    warnings.simplefilter(\"ignore\")\n","\n","warnings.simplefilter(action='ignore', category=FutureWarning)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Me4Vt8rjoYoO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1597358964583,"user_tz":-60,"elapsed":20777,"user":{"displayName":"YUEWEN LI","photoUrl":"","userId":"15636031451830471650"}},"outputId":"e265821b-4d65-480a-dc3c-6be90e6d895e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XxDRUXAaxNgA","colab_type":"code","colab":{}},"source":["def labelencode(data):\n","    labelencoder = LabelEncoder()\n","    #Assigning numerical values and storing in another column\n","    Y = data.iloc[:,-1]\n","    Y = labelencoder.fit_transform(Y)\n","    return Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xnPp_080wBID","colab_type":"code","colab":{}},"source":["import glob\n","test_files = glob.glob('/content/drive/My Drive/TESTING DATASETS/clas_testing_datasets/*.csv')   # create the list of file\n","\n","test_X_ls = []\n","test_Y_ls = []\n","for filename in test_files:\n","    df = pd.read_csv(filename)\n","    Y = labelencode(df)\n","    X = df.iloc[:,:-1]\n","    X = X._get_numeric_data()\n","    test_Y_ls.append(Y)\n","    test_X_ls.append(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSLf0ls7yXLE","colab_type":"code","colab":{}},"source":["# prepare test data\n","\n","def scale(X, y):\n","  '''\n","  scale the test data\n","  '''\n","  scaled_X = preprocessing.MinMaxScaler().fit_transform(X)\n","  X = pd.DataFrame(data = scaled_X, columns = X.columns)\n","\n","  scaled_data = X.copy()\n","  scaled_data['class'] = y\n","\n","  return X, scaled_data\n","\n","def make_seq(inc):\n","  x = []\n","  start = 0\n","  end = 100\n","\n","  while start < end:\n","    start += inc \n","    start = round(start,2)\n","    x.append(start)\n","  return(x)\n","\n","def percentiles(x, b):\n","  # return np.percentile(x, np.linspace(0,100,num = b))\n","  c = 100/b\n","  return np.percentile(x, make_seq(c))\n","\n","def qsa(x, lb, ub, b):\n","  '''\n","  return a list of length b within range (lb, ub) representing feature x\n","  '''\n","  width = (ub - lb) / b\n","  x = np.sort(x)\n","  res = [0] * b\n","  for i in x:\n","    num = int((i - lb) // width)\n","    if num == b:\n","      res[b-1] += 1\n","    else:\n","      res[num] += 1\n","  res = [i / len(x) for i in res]\n","  return res\n","\n","def bin_edge(X, scaled_data, lb, ub, b, qsa_):\n","  '''\n","  return a representation of features in a dataset\n","  '''\n","  col_names = X.columns\n","\n","  pos = scaled_data[scaled_data['class'] == 1]\n","  neg = scaled_data[scaled_data['class'] == 0]\n","  \n","  #scale to range(-10,10)\n","  pos_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(pos[col_names])\n","  pos_scaler = pd.DataFrame(data = pos_scaler, columns = col_names)\n","  neg_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(neg[col_names])\n","  neg_scaler = pd.DataFrame(data = neg_scaler, columns = col_names)\n","\n","  combos = list(itertools.combinations(col_names,2))\n","\n","  training_data = []\n","\n","  for combo in combos:\n","\n","    bin_all = []\n","\n","    if qsa_ == True:\n","      bin_all.extend(qsa(pos_scaler[combo[0]], lb, ub, b)) \n","      bin_all.extend(qsa(neg_scaler[combo[0]], lb, ub, b))\n","      bin_all.extend(qsa(pos_scaler[combo[1]], lb, ub, b))\n","      bin_all.extend(qsa(neg_scaler[combo[1]], lb, ub, b))\n","\n","    else:\n","      # quantile\n","      bin_all.extend(percentiles(pos_scaler[combo[0]], b))\n","      bin_all.extend(percentiles(neg_scaler[combo[0]], b))\n","      bin_all.extend(percentiles(pos_scaler[combo[1]], b))\n","      bin_all.extend(percentiles(neg_scaler[combo[1]], b))\n","\n","    training_data.append(bin_all)\n","\n","  return training_data\n","\n","def create_testdata(X, Y, bins, qsa_):\n","  '''\n","  return test data to be fed into mlps\n","  '''\n","  test_data = []\n","  \n","  X, scaled_data = scale(X, Y)\n","  test = bin_edge(X, scaled_data, -10, 10, 200, True)\n","  test_data.extend(test)\n","  \n","  return test_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"en8UQ6rrHrmS","colab_type":"text"},"source":["# Output recommendation results"]},{"cell_type":"markdown","metadata":{"id":"u4TTvpghysC4","colab_type":"text"},"source":["### output all transformations per dataset"]},{"cell_type":"code","metadata":{"id":"GejD7UzYlwSU","colab_type":"code","colab":{}},"source":["# recommend all\n","def recommend(path, b, X, Y, qsa_):\n","  # iterate over all models\n","  models_ls = glob.glob(path)\n","\n","  # res = []\n","\n","  test_new = X.copy()\n","  # for one test data\n","  test_data = create_testdata(X, Y, b, qsa_)\n","\n","  colnames = X.columns\n","\n","  combos = list(itertools.combinations(colnames,2))\n","\n","  row = []\n","  scores = []\n","\n","  for m in models_ls:\n","    row.append(m.split('/')[-1][:-3])\n","    model = tf.python.keras.models.load_model(m)\n","    pred = model.predict(np.array(test_data), verbose=1)\n","    # the score for positive label\n","    scores.append(pred[:,1])\n","\n","  result = pd.DataFrame(scores, columns = combos, index = row)\n","  \n","  # print(result.head())\n","  # print('Recommended transformation for ',test_files[i].split('/')[-1][:-4])\n","  # print('\\n')\n","\n","  dict_ = {}\n","\n","  cnt = 0\n","  \n","  for combo in combos:\n","    sorted_result = result.sort_values(axis = 0, by = combo, ascending = False)\n","    rcm_trans = sorted_result[sorted_result[combo] >= 0.5].index.tolist()\n","    dict_[combo] = rcm_trans\n","    print(combo,': ',rcm_trans)\n","    if len(rcm_trans) > 0:\n","      cnt += 1\n","\n","  if cnt == 0:\n","    print('No transformation is recommended for this dataset!!!')\n","    \n","  # res.append(dict_)\n","  \n","  # print('\\n')\n","\n","  return dict_, cnt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dt7ruIxAGAge","colab_type":"code","colab":{}},"source":["# classifiers\n","def lr_smote(x_train, y_train):\n","  imba_pipeline = make_pipeline(SMOTE(random_state=42), LogisticRegression(max_iter=500, solver=\"liblinear\", random_state=0))\n","  score = cross_val_score(imba_pipeline, x_train, y_train, scoring='f1', cv=5)\n","  score = np.mean(score)\n","  return(score) \n","def random_forest(data, labels):\n","  rfc = RandomForestClassifier(max_depth=3, random_state=0)\n","  score = cross_val_score(rfc, data, labels, cv=5, scoring='f1')\n","  score = np.mean(score)\n","  return(score)\n","  \n","def rf_smote(x_train, y_train):\n","  imba_pipeline = make_pipeline(SMOTE(random_state=42), \n","                                RandomForestClassifier(max_depth=3, random_state=0))\n","  score = cross_val_score(imba_pipeline, x_train, y_train, scoring='f1', cv=5)\n","  score = np.mean(score)\n","  return(score) \n","  \n","def log_reg(max_iter, X, Y):\n","  \"\"\"Applies logistic regression given x features and vector y of labels \"\"\"\n","  logisticRegr = LogisticRegression(max_iter=max_iter, solver=\"liblinear\", random_state=0)\n","  score = cross_val_score(logisticRegr, X, Y, cv=5, scoring='f1')\n","  score = np.mean(score)\n","  return(score)\n","\n","def smote(X,Y):\n","  \"\"\"Applies smote to fix class imbalance\"\"\"\n","  smt = SMOTE()\n","  x_train, y_train = smt.fit_sample(X, Y)\n","  x = pd.DataFrame(data = x_train, columns = X.columns)\n","  return(x, y_train)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SSpzjRtixWUD","colab_type":"text"},"source":["## transform functions"]},{"cell_type":"code","metadata":{"id":"vBs_kywzVn6F","colab_type":"code","colab":{}},"source":["# binary transformation\n","def add(data, feature1, feature2):\n","  data_new = data.copy()\n","  data_new['new'] = data_new[feature1] + data_new[feature2]\n","  #data_new.drop(axis=0,columns=[feature1,feature2],inplace=True)\n","  return(data_new)\n","\n","def multiply(data, feature1, feature2):\n","  data_new = data.copy()\n","  data_new['new'] = data_new[feature1] * data_new[feature2]\n","  return(data_new)\n","\n","def subtract(data, feature1, feature2):\n","  data_new = data.copy()\n","  data_new['new'] = abs(data_new[feature1] - data_new[feature2])\n","  #data_new.drop(axis=0,columns=[feature1,feature2],inplace=True)\n","  return(data_new)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CdqPxJZ8H7nE","colab_type":"text"},"source":["# Apply transformations"]},{"cell_type":"code","metadata":{"id":"eqJr4DBewDvP","colab_type":"code","colab":{}},"source":["def apply_trans(dict_, X, Y):\n","\n","  score_before_ls = []\n","  score_after_ls = []\n","  diff_ls = []\n","  file_ls = []\n","  \n","  test_original = X.copy()\n","  label = Y\n","\n","  test_original, scaled_data = scale(test_original, label)\n","\n","  colnames = X.columns\n","\n","  # apply transformation\n","  methods = {'add': add, 'subtract':subtract, 'multiply':multiply}\n","  test_new = test_original.copy()\n","\n","  # dict_ = res[i]\n","  \n","  '''\n","  # apply all transformations on a feature\n","  for col in colnames:\n","    if len(dict_[col]) > 0:\n","      for i in dict_[col]:\n","        test_new = methods[i](test_new, col)\n","      test_new.drop(columns=[col],inplace = True)\n","  '''\n","  '''\n","  # apply one transformation on a feature\n","  for col in colnames:\n","    if len(dict_[col]) > 0:\n","      test_new = methods[dict_[col][0]](test_new, col)  \n","  \n","\n","  '''\n","  # corresponding to one transformation per dataset\n","  combo = list(dict_.keys())[0]\n","  trans = dict_[combo]\n","  test_new = methods[trans](test_new, combo[0], combo[1])\n","  \n","  return test_new, label\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpCI3Rw9HoV-","colab_type":"code","colab":{}},"source":["def eval(avg, file_name, test_original, test_new, label, *args):\n","\n","  if avg == True:\n","    score_log_reg0 = log_reg(500, test_original, label)\n","    score_rf0 = random_forest(test_original, label, 5)\n","    score_gb0 = gb(test_original, label, 0.04)\n","\n","    score_before = (score_log_reg0 + score_rf0 + score_gb0) / 3\n","\n","    score_log_reg1 = log_reg(500, test_new, label)\n","    score_rf1 = random_forest(test_new, label, 5)\n","    score_gb1 = gb(test_new, label, 0.04)\n","\n","    score_after = (score_log_reg1 + score_rf1 + score_gb1) / 3\n","\n","  else:\n","    model = args[0]\n","\n","    count_0 = len([1 for y in label if y == 0])\n","    count_1 = len([1 for y in label if y == 1])\n","    min_count = min(count_0,count_1)\n","    \n","\n","    if model == 'lr':\n","      if min_count/len(label) < 0.4: \n","        score_before = lr_smote(test_original, label)\n","        score_after = lr_smote(test_new, label)\n","      else:\n","        score_before = log_reg(500, test_original, label)\n","        score_after = log_reg(500, test_new, label)\n","\n","    elif model == 'rf':\n","      if min_count/len(label) < 0.4: \n","        score_before = rf_smote(test_original, label)\n","        score_after = rf_smote(test_new, label)\n","      else:\n","        score_before = random_forest(test_original, label)\n","        score_after = random_forest(test_new, label)\n","\n","    elif model == 'gb':\n","      score_before = gb(test_original, label, 0.02)\n","      score_after = gb(test_new, label, 0.02)\n","\n","  \n","  if score_after > score_before:\n","    improve = ' improved'\n","  else:\n","    improve = ' not improved'\n","\n","  diff = score_after - score_before\n","\n","  print(file_name, ' original score: ',score_before,'; score after: ',score_after, '; difference: ', diff ,improve)\n","  # print(test_files[i].split('/')[-1][:-4], improve)\n","\n","  return score_before, score_after, diff, file_name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ch9UHBq1G0EW","colab_type":"code","colab":{}},"source":["# combanation of trasformations\n","def multiple_testing(path, b, n, model, qsa_):\n","  score_before_ls = []\n","  score_after_ls = []\n","  diff_ls = []\n","  file_ls = []\n","\n","  for i in range(len(test_files)):\n","    X = test_X_ls[i].copy()\n","    Y = test_Y_ls[i].copy()\n","    X_original, X_scaled = scale(X,Y)\n","    for j in range(n):\n","      res, cnt = recommend(path, b, X, Y, qsa_)\n","      if cnt > 0:\n","        X_new, label = apply_trans(res, X, Y)\n","        X = X_new\n","      else:\n","        X = X_original\n","    score_before, score_after, diff, file = eval(False, test_files[i].split('/')[-1][:-4], X_original, X, test_Y_ls[i], model)\n","    # if score_before <= 0.999 and score_before > 0:\n","    score_before_ls.append(score_before)\n","    score_after_ls.append(score_after)\n","    diff_ls.append(score_after - score_before)\n","    file_ls.append(test_files[i].split('/')[-1][:-4])\n","\n","  return score_before_ls, score_after_ls, diff_ls, file_ls\n"],"execution_count":null,"outputs":[]}]}