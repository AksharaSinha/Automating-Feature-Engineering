{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Testing binary regression.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"3TUJShYLjqgX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1597394956304,"user_tz":-60,"elapsed":3330,"user":{"displayName":"YUEWEN LI","photoUrl":"","userId":"15636031451830471650"}},"outputId":"54bdf750-2f06-4e64-e59d-4eff512e84e9"},"source":["from sklearn import preprocessing\n","from sklearn.preprocessing import (LabelEncoder, KBinsDiscretizer, StandardScaler, MinMaxScaler)\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.utils import np_utils\n","from keras.layers.core import Dense, Activation, Dropout\n","from keras.optimizers import Adam, Nadam\n","from sklearn.utils import class_weight\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","from keras.callbacks import ModelCheckpoint\n","import imblearn\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import BorderlineSMOTE\n","from sklearn.metrics import confusion_matrix\n","import sklearn\n","import tensorflow as tf\n","from google.colab import files\n","import pickle\n","from sklearn.linear_model import (LogisticRegression, LogisticRegressionCV)\n","from sklearn.datasets import make_classification\n","from sklearn.ensemble import (GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor)\n","from imblearn.over_sampling import SMOTE\n","import collections\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import cross_val_score\n","import itertools\n","import random\n","from random import sample\n","from pandas import DataFrame\n","from sklearn.cluster import KMeans\n","from sklearn.linear_model import LinearRegression\n","from sklearn import metrics\n","\n","import warnings\n","import sys\n","\n","if not sys.warnoptions:\n","    warnings.simplefilter(\"ignore\")\n","\n","warnings.simplefilter(action='ignore', category=FutureWarning)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Me4Vt8rjoYoO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1597394990156,"user_tz":-60,"elapsed":16327,"user":{"displayName":"YUEWEN LI","photoUrl":"","userId":"15636031451830471650"}},"outputId":"f01720aa-88ba-45a6-a414-a05e260ac566"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XxDRUXAaxNgA","colab_type":"code","colab":{}},"source":["def get_data(list_of_files):\n","  X_ls = []\n","  Y_ls = []\n","  Y_label = []\n","  for filename in list_of_files:\n","    if filename[-1] == 'x':\n","      df = pd.read_excel(filename)\n","    else:\n","      df = pd.read_csv(filename)\n","    df = df.dropna()\n","\n","    if len(df.columns) > 2:\n","\n","      Y = df.iloc[:,-1]\n","      Y = np.array(Y).reshape(-1, 1)\n","      scaler = MinMaxScaler()\n","      Y = pd.DataFrame(scaler.fit_transform(Y))\n","      Y_ls.append(Y)\n","      \n","      # getting Y_label\n","      yd = DataFrame(Y)\n","      y = yd.values\n","      kmeans = KMeans(n_clusters=6).fit(y)\n","      classified_data = kmeans.labels_\n","      df_processed = yd.copy()\n","      df_processed['Cluster Class'] = pd.Series(classified_data, index=df_processed.index)\n","      y_class = df_processed.iloc[:,-1]\n","      Y_label.append(y_class)\n","\n","      # getting X_ls\n","      X = df.iloc[:, :-1]\n","      X = X._get_numeric_data()\n","      X_ls.append(X)\n","\n","  return(X_ls,Y_ls,Y_label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WdlF296rKXmv","colab_type":"code","colab":{}},"source":["# for cp\n","def get_data(list_of_files):\n","  X_ls = []\n","  Y_ls = []\n","  \n","  for filename in list_of_files:\n","    if filename[-1] == 'x':\n","      df = pd.read_excel(filename)\n","    else:\n","      df = pd.read_csv(filename)\n","    df = df.dropna()\n","\n","    # getting Y_ls\n","    Y = df.iloc[:,-1]\n","    Y = np.array(Y).reshape(-1, 1)\n","    scaler = MinMaxScaler()\n","    Y = pd.DataFrame(scaler.fit_transform(Y))\n","    Y_ls.append(Y)\n","\n","    # getting X_ls\n","    X = df.iloc[:, :-1]\n","    X = X._get_numeric_data()\n","    X_ls.append(X)\n","\n","  return(X_ls,Y_ls)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xnPp_080wBID","colab_type":"code","colab":{}},"source":["import glob\n","test_files = glob.glob('/content/drive/My Drive/TESTING DATASETS/reg_binary/*.csv')\n","test_files_xlsx = glob.glob('/content/drive/My Drive/TESTING DATASETS/reg_binary/*.xlsx')\n","test_files.extend(test_files_xlsx)   # create the list of file\n","\n","# X_ls, Y_ls, Y_label = get_data(test_files)\n","X_ls, Y_ls = get_data(test_files)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSLf0ls7yXLE","colab_type":"code","colab":{}},"source":["# prepare test data\n","\n","def scale(X, y):\n","  '''\n","  scale the test data\n","  '''\n","  scaled_X = preprocessing.MinMaxScaler().fit_transform(X)\n","  X = pd.DataFrame(data = scaled_X, columns = X.columns)\n","\n","  scaled_data = X.copy()\n","  scaled_data['class'] = y\n","\n","  return X, scaled_data\n","\n","def make_seq(inc):\n","  x = []\n","  start = 0\n","  end = 100\n","\n","  while start < end:\n","    start += inc \n","    start = round(start,2)\n","    x.append(start)\n","  return(x)\n","\n","def percentiles(x, b):\n","  # return np.percentile(x, np.linspace(0,100,num = b))\n","  c = 100/b\n","  return np.percentile(x, make_seq(c))\n","\n","def qsa(x, b):\n","  '''\n","  return a list of length b within range (lb, ub) representing feature x\n","  '''\n","  width = 20 / b\n","  x = np.sort(x)\n","  res = [0] * b\n","  for i in x:\n","    num = int((i - -10) // width)\n","    if num == b:\n","      res[b-1] += 1\n","    else:\n","      res[num] += 1\n","  res = [i / len(x) for i in res]\n","  return res\n","\n","def bin_edge(X, scaled_data, lb, ub, b):\n","  '''\n","  return a representation of features in a dataset\n","  '''\n","  col_names = X.columns\n","\n","  cls0 = scaled_data[scaled_data['class'] == 0]\n","  cls1 = scaled_data[scaled_data['class'] == 1]\n","  cls2 = scaled_data[scaled_data['class'] == 2]\n","  cls3 = scaled_data[scaled_data['class'] == 3]\n","  cls4 = scaled_data[scaled_data['class'] == 4]\n","  cls5 = scaled_data[scaled_data['class'] == 5]\n","  \n","  #scale to range(-10,10)\n","  cls0_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(cls0[col_names])\n","  cls0_scaler = pd.DataFrame(data = cls0_scaler, columns = col_names)\n","  cls1_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(cls1[col_names])\n","  cls1_scaler = pd.DataFrame(data = cls1_scaler, columns = col_names)\n","  cls2_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(cls2[col_names])\n","  cls2_scaler = pd.DataFrame(data = cls2_scaler, columns = col_names)\n","  cls3_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(cls3[col_names])\n","  cls3_scaler = pd.DataFrame(data = cls3_scaler, columns = col_names)\n","  cls4_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(cls4[col_names])\n","  cls4_scaler = pd.DataFrame(data = cls4_scaler, columns = col_names)\n","  cls5_scaler = MinMaxScaler(feature_range=(lb, ub)).fit_transform(cls5[col_names])\n","  cls5_scaler = pd.DataFrame(data = cls5_scaler, columns = col_names)\n","\n","  if len(col_names) > 1:\n","    combos = list(itertools.combinations(col_names,2))\n","  else:\n","    combos = []\n","\n","  training_data = []\n","  \n","  for combo in combos:\n","\n","    bin_all = []\n","\n","    bin_all.extend(qsa(cls0_scaler[combo[0]], b))\n","    bin_all.extend(qsa(cls1_scaler[combo[0]], b))\n","    bin_all.extend(qsa(cls2_scaler[combo[0]], b))\n","    bin_all.extend(qsa(cls3_scaler[combo[0]], b))\n","    bin_all.extend(qsa(cls4_scaler[combo[0]], b))\n","    bin_all.extend(qsa(cls5_scaler[combo[0]], b))\n","\n","    bin_all.extend(qsa(cls0_scaler[combo[1]], b))\n","    bin_all.extend(qsa(cls1_scaler[combo[1]], b))\n","    bin_all.extend(qsa(cls2_scaler[combo[1]], b))\n","    bin_all.extend(qsa(cls3_scaler[combo[1]], b))\n","    bin_all.extend(qsa(cls4_scaler[combo[1]], b))\n","    bin_all.extend(qsa(cls5_scaler[combo[1]], b))\n","    \n","    training_data.append(bin_all)\n","\n","  return training_data\n","\n","def create_testdata(X, Y, bins):\n","  '''\n","  return test data to be fed into mlps\n","  '''\n","  test_data = []\n","  \n","  X, scaled_data = scale(X, Y)\n","  test = bin_edge(X, scaled_data, -10, 10, bins)\n","  test_data.extend(test)\n","  \n","  return test_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcNrz5GoKrKR","colab_type":"code","colab":{}},"source":["# prepare test data for cp\n","\n","def scale(variable):\n","  \"\"\"Scales the original variable from 0 to 1\"\"\"\n","  data = np.array(variable).reshape(-1,1)\n","  scaled_data = preprocessing.MinMaxScaler().fit_transform(data)\n","  scaled_data = pd.DataFrame(data = scaled_data)\n","  scaled_data = scaled_data.copy()\n","  return(scaled_data)\n","\n","def scale_data(X):\n","  '''\n","  scale the test data\n","  '''\n","  scaled_X = preprocessing.MinMaxScaler().fit_transform(X)\n","  X = pd.DataFrame(data = scaled_X, columns = X.columns)\n","\n","  return X\n","\n","def ranging(variable, bins):\n","  quantiles_list = []\n","  quantiles = make_seq(inc=bins)\n","  for i in quantiles:\n","    quantiles_list.append(np.percentile(variable, i))\n","  return(quantiles_list)\n","\n","def make_seq(inc):\n","  x = []\n","  start = 0\n","  end = 100\n","\n","  while start < end:\n","    start += inc \n","    start = round(start,2)\n","    x.append(start)\n","  return(x)\n","\n","def conditional_binning(data,Y,X_q,Y_q):\n","  FXY = []\n","  FY = []\n","\n","  oldy_q = 0\n","  for y_q in Y_q:\n","    if oldy_q == 0:\n","      count = sum(1 for y in Y if y <= y_q and y >= oldy_q )\n","    else:\n","      count = sum(1 for y in Y if y <= y_q and y > oldy_q )\n","    oldy_q = y_q\n","    FY.append(count)\n","\n","\n","  oldx_q = 0\n","  for x_q in X_q:\n","    oldy_q = 0\n","    for i,y_q in enumerate(Y_q):\n","      \n","      if oldx_q == 0 and oldy_q == 0:\n","        count = sum(1 for i in range(0,len(data)) if data[i] <= x_q and data[i] >= oldx_q and Y[i] <= y_q and Y[i] >= oldy_q )\n","        \n","      elif oldx_q == 0:\n","        count = sum(1 for i in range(0,len(data)) if data[i] <= x_q and data[i] >= oldx_q and Y[i] <= y_q and Y[i] > oldy_q )\n","        \n","      elif oldy_q == 0:\n","        count = sum(1 for i in range(0,len(data)) if data[i] <= x_q and data[i] > oldx_q and Y[i] <= y_q and Y[i] >= oldy_q )\n","        \n","      else:\n","        count = sum(1 for i in range(0,len(data)) if data[i] <= x_q and data[i] > oldx_q and Y[i] <= y_q and Y[i] > oldy_q )\n","      if FY[i] != 0:\n","        FXY.append(count/FY[i])\n","      else:\n","        FXY.append(0)\n","      oldy_q = y_q\n","    oldx_q = x_q\n","\n","  return(FXY)\n","\n","def conditional_binning(data,Y,X_q,Y_q):\n","  FXY = []\n","  FY = []\n","\n","  for y_q in Y_q:\n","    count = sum(1 for y in Y if y <= y_q )\n","    FY.append(count)\n","\n","  for x_q, i, y_q in [(x_q, i, y_q) for x_q in X_q for i, y_q in enumerate(Y_q)]:\n","    count = sum(1 for x,y in zip(data, Y) if x <= x_q and y <= y_q)\n","        \n","    if FY[i] != 0:\n","      FXY.append(count/FY[i])\n","    else:\n","      FXY.append(0)\n","      \n","\n","  return(FXY)\n","  \n","def binning(features_list, data, y):\n","    \n","  b = {\"bin_edges\":[]}\n","\n","  y = y[0].to_list()\n","  Y_q = ranging(y,5)\n","\n","  if len(features_list) > 1:\n","    combos = list(itertools.combinations(features_list,2))\n","  else:\n","    combos = []\n","\n","  for combo in combos:\n","    bin_all = []\n","\n","    data_feature = list(data[combo[0]])\n","    data_feature = scale(data_feature)\n","    data_feature = data_feature[0].to_list()\n","    \n","    X_q = ranging(data_feature,5)\n","    \n","    bin = conditional_binning(data_feature,y,X_q,Y_q)\n","    bin_all.extend(bin)\n","\n","    data_feature = list(data[combo[1]])\n","    data_feature = scale(data_feature)\n","    data_feature = data_feature[0].to_list()\n","    \n","    X_q = ranging(data_feature,5)\n","    \n","    bin = conditional_binning(data_feature,y,X_q,Y_q)\n","    bin_all.extend(bin)\n","\n","    b[\"bin_edges\"].append(bin_all) \n","    \n","  training_data = np.array(b[\"bin_edges\"])\n"," \n","  return(training_data)\n","  \n","def create_testdata(X, Y):\n","  '''\n","  return test data to be fed into mlps\n","  '''\n","  test_data = []\n","  \n","  features_list = X.columns\n","  test = binning(features_list, X, Y)\n","  test_data.extend(test)\n","  \n","  return test_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"en8UQ6rrHrmS","colab_type":"text"},"source":["# Output recommendation results"]},{"cell_type":"markdown","metadata":{"id":"u4TTvpghysC4","colab_type":"text"},"source":["### output all transformations per dataset"]},{"cell_type":"code","metadata":{"id":"GejD7UzYlwSU","colab_type":"code","colab":{}},"source":["# recommend all\n","def recommend(path, b, X, Y, qsa_):\n","  # iterate over all models\n","  models_ls = glob.glob(path)\n","\n","  # res = []\n","\n","  test_new = X.copy()\n","  # for one test data\n","  test_data = create_testdata(X, Y, b, qsa_)\n","\n","  colnames = X.columns\n","\n","  combos = list(itertools.combinations(colnames,2))\n","\n","  row = []\n","  scores = []\n","\n","  for m in models_ls:\n","    row.append(m.split('/')[-1][:-3])\n","    model = tf.python.keras.models.load_model(m)\n","    pred = model.predict(np.array(test_data), verbose=1)\n","    # the score for positive label\n","    scores.append(pred[:,1])\n","\n","  result = pd.DataFrame(scores, columns = combos, index = row)\n","  \n","  # print(result.head())\n","  # print('Recommended transformation for ',test_files[i].split('/')[-1][:-4])\n","  # print('\\n')\n","\n","  dict_ = {}\n","\n","  cnt = 0\n","  \n","  for combo in combos:\n","    sorted_result = result.sort_values(axis = 0, by = combo, ascending = False)\n","    rcm_trans = sorted_result[sorted_result[combo] >= 0.5].index.tolist()\n","    dict_[combo] = rcm_trans\n","    print(combo,': ',rcm_trans)\n","    if len(rcm_trans) > 0:\n","      cnt += 1\n","\n","  if cnt == 0:\n","    print('No transformation is recommended for this dataset!!!')\n","    \n","  # res.append(dict_)\n","  \n","  # print('\\n')\n","\n","  return dict_, cnt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dt7ruIxAGAge","colab_type":"code","colab":{}},"source":["# classifiers\n","def linear_regression(x_var, y_var):\n","  X_train, X_test, y_train, y_test = train_test_split(x_var, y_var, test_size=0.2, random_state=0)\n","  lr = LinearRegression()  \n","  lr.fit(X_train, y_train)\n","  y_pred = lr.predict(X_test)\n","  score = metrics.r2_score(y_test, y_pred)\n","  return(score)\n","\n","def decision_tree(x_var, y_var):\n","  X_train, X_test, y_train, y_test = train_test_split(x_var, y_var, test_size=0.2, random_state=0)\n","  clf = tree.DecisionTreeRegressor(random_state = 0)\n","  clf.fit(X_train,y_train)\n","  y_pred = clf.predict(X_test)\n","  score = metrics.r2_score(y_test, y_pred)\n","  return(score)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SSpzjRtixWUD","colab_type":"text"},"source":["## Replace features"]},{"cell_type":"code","metadata":{"id":"vBs_kywzVn6F","colab_type":"code","colab":{}},"source":["# binary transformation\n","def add(data, feature1, feature2):\n","  data_new = data.copy()\n","  data_new['new'] = data_new[feature1] + data_new[feature2]\n","  data_new.drop(axis=0,columns=[feature1,feature2],inplace=True)\n","  return(data_new)\n","\n","def multiply(data, feature1, feature2):\n","  data_new = data.copy()\n","  data_new['new'] = data_new[feature1] * data_new[feature2]\n","  #data_new.drop(axis=0,columns=[feature1,feature2],inplace=True)\n","  return(data_new)\n","\n","def subtract(data, feature1, feature2):\n","  data_new = data.copy()\n","  data_new['new'] = abs(data_new[feature1] - data_new[feature2])\n","  #data_new.drop(axis=0,columns=[feature1,feature2],inplace=True)\n","  return(data_new)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Z4C5exsxaQK","colab_type":"text"},"source":["## Add features"]},{"cell_type":"code","metadata":{"id":"7WG1Kl_exU_J","colab_type":"code","colab":{}},"source":["# transformations\n","def freq_add(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  given_feature = data_new[feature]\n","  frequency = collections.Counter(given_feature) \n","  transformed_feature = [frequency[x] for x in given_feature] \n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)\n","\n","def sq_add(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = transformed_feature**2\n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)\n","\n","def sqr_add(data, feature):\n","  \"\"\"convert the feature values to absolute value and then take the square root\"\"\"\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = abs(transformed_feature)\n","  transformed_feature = np.sqrt(transformed_feature)\n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)\n","\n","def log_add(data, feature):\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = abs(transformed_feature)\n","  transformed_feature = np.where(transformed_feature == 0, 1e-9, transformed_feature)\n","  transformed_feature = np.log(transformed_feature)\n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)\n","\n","def sigmoid_add(data, feature):\n","  data_new = data.copy()\n","  transformed_feature = data_new[feature]\n","  transformed_feature = 1/(1 + np.exp(-transformed_feature))\n","  feature_new = feature+'_new'\n","  data_new[feature_new] = transformed_feature\n","  return(data_new)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CdqPxJZ8H7nE","colab_type":"text"},"source":["# Apply transformations"]},{"cell_type":"code","metadata":{"id":"eqJr4DBewDvP","colab_type":"code","colab":{}},"source":["def apply_trans(dict_, X, Y):\n","\n","  score_before_ls = []\n","  score_after_ls = []\n","  diff_ls = []\n","  file_ls = []\n","  \n","  test_original = X.copy()\n","  label = Y\n","\n","  test_original = scale_data(test_original)\n","\n","  colnames = X.columns\n","\n","  # apply transformation\n","  methods = {'add': add, 'subtract':subtract, 'multiply':multiply}\n","  test_new = test_original.copy()\n","\n","  # dict_ = res[i]\n","  \n","  '''\n","  # apply all transformations on a feature\n","  for col in colnames:\n","    if len(dict_[col]) > 0:\n","      for i in dict_[col]:\n","        test_new = methods[i](test_new, col)\n","      test_new.drop(columns=[col],inplace = True)\n","  '''\n","  '''\n","  # apply one transformation on a feature\n","  for col in colnames:\n","    if len(dict_[col]) > 0:\n","      test_new = methods[dict_[col][0]](test_new, col)  \n","  \n","\n","  '''\n","  # corresponding to one transformation per dataset\n","  combo = list(dict_.keys())[0]\n","  trans = dict_[combo]\n","  test_new = methods[trans](test_new, combo[0], combo[1])\n","  \n","  return test_new, label\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpCI3Rw9HoV-","colab_type":"code","colab":{}},"source":["def eval(avg, file_name, test_original, test_new, label, *args):\n","\n","  if avg == True:\n","    score_lnr0 = linear_regression(test_original, label)\n","    score_dtr0 = decision_tree(test_original, label)\n","    \n","    score_before = (score_lnr0 + score_dtr0) / 2\n","\n","    score_lnr1 = linear_regression(test_new, label)\n","    score_dtr1 = decision_tree(test_new, label)\n","    \n","    score_after = (score_lnr1 + score_dtr1) / 2\n","\n","  else:\n","    model = args[0]\n","    if model == 'lnr':\n","      score_before = linear_regression(test_original, label)\n","      score_after = linear_regression(test_new, label)\n","\n","    elif model == 'dtr':\n","      score_before = decision_tree(test_original, label)\n","      score_after = decision_tree(test_new, label)\n","  \n","  if score_after > score_before:\n","    improve = ' improved'\n","  else:\n","    improve = ' not improved'\n","\n","  diff = score_after - score_before\n","\n","  print(file_name, ' original score: ',score_before,'; score after: ',score_after, '; difference: ', diff ,improve)\n","  # print(test_files[i].split('/')[-1][:-4], improve)\n","\n","  return score_before, score_after, diff, file_name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ch9UHBq1G0EW","colab_type":"code","colab":{}},"source":["# combination of trasformations\n","def multiple_testing(path, b, n, model):\n","  score_before_ls = []\n","  score_after_ls = []\n","  diff_ls = []\n","  file_ls = []\n","\n","  for i in range(len(X_ls)):\n","    X = X_ls[i].copy()\n","    Y = Y_ls[i].copy()\n","    #label = Y_label[i].copy()\n","    X_origin = scale_data(X)\n","    for j in range(n):\n","      #res, cnt = recommend(path, b, X, label)\n","      res, cnt = recommend(path, b, X, Y)\n","      if cnt > 0:\n","        X_new, Y = apply_trans(res, X, Y)\n","        X = X_new\n","    score_before, score_after, diff, file = eval(False, test_files[i].split('/')[-1][:-4], X_origin, X, Y, model)\n","    #if score_before <= 0.999 and score_before > 0:\n","    score_before_ls.append(score_before)\n","    score_after_ls.append(score_after)\n","    diff_ls.append(score_after - score_before)\n","    file_ls.append(test_files[i].split('/')[-1][:-4])\n","\n","  return score_before_ls, score_after_ls, diff_ls, file_ls\n"],"execution_count":null,"outputs":[]}]}